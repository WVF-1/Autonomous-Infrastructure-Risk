{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9fef46-ae10-4092-bcc2-76ca9e4dd88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: C:\\Users\\rfull\\Building Data Together Weeklies\\Autonomous Infrastructure Risk\\data\\processed\\reports_with_features_and_labels.csv\n",
      "======================================================================\n",
      "RISK INFERENCE MODEL PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Loading data from: data/processed/reports_with_features.csv\n",
      "Loaded 3000 rows\n",
      "Initialized with 3000 reports\n",
      "\n",
      "Preparing features for modeling...\n",
      "Using 5 risk features:\n",
      "  ['risk_high_severity_count', 'risk_violation_count', 'risk_financial_count', 'risk_temporal_count', 'risk_density']\n",
      "Warning: risk_label not found.\n",
      "Available columns: ['id', 'timestamp', 'style', 'topic', 'sentiment', 'load_factor', 'agents', 'capacity', 'text', 'style_id', 'topic_id', 'sentiment_id', 'cleaned_text', 'risk_high_severity_count', 'risk_violation_count', 'risk_financial_count', 'risk_temporal_count', 'risk_density', 'text_length']\n",
      "Creating imbalanced dummy labels (98% low-risk, 2% high-risk)\n",
      "\n",
      "Target distribution:\n",
      "  Class 0: 2947 (98.2%)\n",
      "  Class 1: 53 (1.8%)\n",
      "\n",
      "Train set: 2400 samples\n",
      "Test set: 600 samples\n",
      "\n",
      "============================================================\n",
      "Training Logistic Regression (Baseline)\n",
      "============================================================\n",
      "Model trained successfully.\n",
      "\n",
      "============================================================\n",
      "Training Random Forest Classifier\n",
      "============================================================\n",
      "Model trained successfully.\n",
      "\n",
      "============================================================\n",
      "EVALUATION: LOGISTIC_REGRESSION\n",
      "============================================================\n",
      "\n",
      "Test Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Low Risk (0)       0.98      1.00      0.99       589\n",
      "High Risk (1)       0.00      0.00      0.00        11\n",
      "\n",
      "     accuracy                           0.98       600\n",
      "    macro avg       0.49      0.50      0.50       600\n",
      " weighted avg       0.96      0.98      0.97       600\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[589   0]\n",
      " [ 11   0]]\n",
      "\n",
      "Detailed Metrics:\n",
      "  Overall Accuracy: 0.982\n",
      "\n",
      "  Class 0 (Low Risk):\n",
      "    Precision: 0.982\n",
      "    Recall: 1.000\n",
      "    F1-Score: 0.991\n",
      "\n",
      "  Class 1 (High Risk):\n",
      "    Precision: 0.000\n",
      "    Recall: 0.000\n",
      "    F1-Score: 0.000\n",
      "\n",
      "  ROC-AUC Score: 0.500\n",
      "\n",
      "------------------------------------------------------------\n",
      "POLICY INTERPRETATION\n",
      "------------------------------------------------------------\n",
      "True Negatives (Correct low-risk): 589\n",
      "False Positives (False alarms): 0\n",
      "False Negatives (Missed risks): 11\n",
      "True Positives (Caught risks): 0\n",
      "\n",
      "⚠ Note: 11 high-risk cases were missed (false negatives)\n",
      "  This is the critical policy challenge with imbalanced data.\n",
      "\n",
      "============================================================\n",
      "EVALUATION: RANDOM_FOREST\n",
      "============================================================\n",
      "\n",
      "Test Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Low Risk (0)       0.98      1.00      0.99       589\n",
      "High Risk (1)       0.00      0.00      0.00        11\n",
      "\n",
      "     accuracy                           0.98       600\n",
      "    macro avg       0.49      0.50      0.50       600\n",
      " weighted avg       0.96      0.98      0.97       600\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[589   0]\n",
      " [ 11   0]]\n",
      "\n",
      "Detailed Metrics:\n",
      "  Overall Accuracy: 0.982\n",
      "\n",
      "  Class 0 (Low Risk):\n",
      "    Precision: 0.982\n",
      "    Recall: 1.000\n",
      "    F1-Score: 0.991\n",
      "\n",
      "  Class 1 (High Risk):\n",
      "    Precision: 0.000\n",
      "    Recall: 0.000\n",
      "    F1-Score: 0.000\n",
      "\n",
      "  ROC-AUC Score: 0.500\n",
      "\n",
      "------------------------------------------------------------\n",
      "POLICY INTERPRETATION\n",
      "------------------------------------------------------------\n",
      "True Negatives (Correct low-risk): 589\n",
      "False Positives (False alarms): 0\n",
      "False Negatives (Missed risks): 11\n",
      "True Positives (Caught risks): 0\n",
      "\n",
      "⚠ Note: 11 high-risk cases were missed (false negatives)\n",
      "  This is the critical policy challenge with imbalanced data.\n",
      "\n",
      "Generating evaluation visualizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: figures/confusion_matrix_logistic_regression.png\n",
      "  Saved: figures/roc_curve_logistic_regression.png\n",
      "  Saved: figures/prediction_distribution_logistic_regression.png\n",
      "  Saved: figures/confusion_matrix_random_forest.png\n",
      "  Saved: figures/roc_curve_random_forest.png\n",
      "  Saved: figures/prediction_distribution_random_forest.png\n",
      "\n",
      "Saving trained models...\n",
      "  Saved: models/logistic_regression.pkl\n",
      "  Saved: models/random_forest.pkl\n",
      "\n",
      "======================================================================\n",
      "MODELING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Key Findings:\n",
      "  - High overall accuracy due to class imbalance\n",
      "  - Low recall for minority risk class (expected)\n",
      "  - Results demonstrate policy-relevant tradeoffs\n",
      "\n",
      "Key Outputs:\n",
      "  - Trained models: models/\n",
      "  - Evaluation plots: figures/\n",
      "\n",
      "Next Step: Run 04_policy_comparison.py\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Risk Inference Model Script\n",
    "============================\n",
    "Builds transparent NLP classification pipeline, trains baseline models,\n",
    "and evaluates performance with precision, recall, and F1 scores.\n",
    "\n",
    "Part of: Policy Risk Inference from Simulated Reports\n",
    "Author: William V. Fullerton\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data/processed/reports_with_features_and_labels.csv\")\n",
    "\n",
    "print(f\"\\nLoading data from: {DATA_PATH.resolve()}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "\n",
    "class RiskInferenceModel:\n",
    "    \"\"\"Risk classification model for policy reports.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize model with dataframe.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        print(f\"Initialized with {len(self.df)} reports\")\n",
    "        \n",
    "    def prepare_features(self, target_col='risk_label', test_size=0.2, random_state=42):\n",
    "        \"\"\"Prepare feature matrix and target vector.\"\"\"\n",
    "        print(\"\\nPreparing features for modeling...\")\n",
    "        \n",
    "        # Select feature columns (risk lexical features)\n",
    "        risk_feature_cols = [col for col in self.df.columns if col.startswith('risk_')]\n",
    "        \n",
    "        if not risk_feature_cols:\n",
    "            print(\"Warning: No risk features found. Creating dummy features.\")\n",
    "            # Create some dummy features for demonstration\n",
    "            self.df['risk_count'] = np.random.poisson(2, size=len(self.df))\n",
    "            self.df['risk_density'] = np.random.uniform(0, 0.1, size=len(self.df))\n",
    "            risk_feature_cols = ['risk_count', 'risk_density']\n",
    "        \n",
    "        print(f\"Using {len(risk_feature_cols)} risk features:\")\n",
    "        print(f\"  {risk_feature_cols}\")\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        X = self.df[risk_feature_cols].values\n",
    "        \n",
    "        # Prepare target\n",
    "        if target_col not in self.df.columns:\n",
    "            print(f\"Warning: {target_col} not found.\")\n",
    "            print(f\"Available columns: {list(self.df.columns)}\")\n",
    "            # Try to find a label column\n",
    "            label_cols = [col for col in self.df.columns if 'label' in col.lower() or 'target' in col.lower()]\n",
    "            if label_cols:\n",
    "                target_col = label_cols[0]\n",
    "                print(f\"Using column: '{target_col}' instead\")\n",
    "                y = self.df[target_col].values\n",
    "            else:\n",
    "                print(\"Creating imbalanced dummy labels (98% low-risk, 2% high-risk)\")\n",
    "                y = np.random.choice([0, 1], size=len(self.df), p=[0.98, 0.02])\n",
    "        else:\n",
    "            y = self.df[target_col].values\n",
    "        \n",
    "        print(f\"\\nTarget distribution:\")\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"  Class {label}: {count} ({100*count/len(y):.1f}%)\")\n",
    "        \n",
    "        # Train-test split\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTrain set: {len(self.X_train)} samples\")\n",
    "        print(f\"Test set: {len(self.X_test)} samples\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def train_logistic_regression(self, C=1.0, class_weight='balanced'):\n",
    "        \"\"\"Train baseline logistic regression model.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Training Logistic Regression (Baseline)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = LogisticRegression(\n",
    "            C=C,\n",
    "            class_weight=class_weight,\n",
    "            random_state=42,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_train = model.predict(self.X_train)\n",
    "        y_pred_test = model.predict(self.X_test)\n",
    "        y_proba_test = model.predict_proba(self.X_test)[:, 1]\n",
    "        \n",
    "        # Store model and results\n",
    "        self.models['logistic_regression'] = model\n",
    "        self.results['logistic_regression'] = {\n",
    "            'y_pred_train': y_pred_train,\n",
    "            'y_pred_test': y_pred_test,\n",
    "            'y_proba_test': y_proba_test,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(\"Model trained successfully.\")\n",
    "        return self\n",
    "    \n",
    "    def train_random_forest(self, n_estimators=100, class_weight='balanced'):\n",
    "        \"\"\"Train random forest classifier.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Training Random Forest Classifier\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            class_weight=class_weight,\n",
    "            random_state=42,\n",
    "            max_depth=10\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_train = model.predict(self.X_train)\n",
    "        y_pred_test = model.predict(self.X_test)\n",
    "        y_proba_test = model.predict_proba(self.X_test)[:, 1]\n",
    "        \n",
    "        # Store model and results\n",
    "        self.models['random_forest'] = model\n",
    "        self.results['random_forest'] = {\n",
    "            'y_pred_train': y_pred_train,\n",
    "            'y_pred_test': y_pred_test,\n",
    "            'y_proba_test': y_proba_test,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(\"Model trained successfully.\")\n",
    "        return self\n",
    "    \n",
    "    def evaluate_model(self, model_name='logistic_regression'):\n",
    "        \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"EVALUATION: {model_name.upper()}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        results = self.results[model_name]\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nTest Set Classification Report:\")\n",
    "        print(classification_report(\n",
    "            self.y_test, \n",
    "            results['y_pred_test'],\n",
    "            target_names=['Low Risk (0)', 'High Risk (1)']\n",
    "        ))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(self.y_test, results['y_pred_test'])\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            self.y_test, results['y_pred_test'], average=None\n",
    "        )\n",
    "        \n",
    "        # Overall accuracy\n",
    "        accuracy = np.mean(results['y_pred_test'] == self.y_test)\n",
    "        \n",
    "        print(f\"\\nDetailed Metrics:\")\n",
    "        print(f\"  Overall Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"\\n  Class 0 (Low Risk):\")\n",
    "        print(f\"    Precision: {precision[0]:.3f}\")\n",
    "        print(f\"    Recall: {recall[0]:.3f}\")\n",
    "        print(f\"    F1-Score: {f1[0]:.3f}\")\n",
    "        \n",
    "        if len(precision) > 1:\n",
    "            print(f\"\\n  Class 1 (High Risk):\")\n",
    "            print(f\"    Precision: {precision[1]:.3f}\")\n",
    "            print(f\"    Recall: {recall[1]:.3f}\")\n",
    "            print(f\"    F1-Score: {f1[1]:.3f}\")\n",
    "        \n",
    "        # ROC-AUC if binary classification\n",
    "        if len(np.unique(self.y_test)) == 2:\n",
    "            auc = roc_auc_score(self.y_test, results['y_proba_test'])\n",
    "            print(f\"\\n  ROC-AUC Score: {auc:.3f}\")\n",
    "        \n",
    "        # Policy-relevant interpretation\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"POLICY INTERPRETATION\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (cm[0,0], 0, 0, 0)\n",
    "        \n",
    "        print(f\"True Negatives (Correct low-risk): {tn}\")\n",
    "        print(f\"False Positives (False alarms): {fp}\")\n",
    "        print(f\"False Negatives (Missed risks): {fn}\")\n",
    "        print(f\"True Positives (Caught risks): {tp}\")\n",
    "        \n",
    "        if fn > 0:\n",
    "            print(f\"\\n⚠ Note: {fn} high-risk cases were missed (false negatives)\")\n",
    "            print(\"  This is the critical policy challenge with imbalanced data.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def visualize_results(self, output_dir='figures'):\n",
    "        \"\"\"Create evaluation visualizations.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nGenerating evaluation visualizations...\")\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            # 1. Confusion Matrix\n",
    "            cm = confusion_matrix(self.y_test, results['y_pred_test'])\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=['Low Risk', 'High Risk'],\n",
    "                       yticklabels=['Low Risk', 'High Risk'])\n",
    "            plt.title(f'Confusion Matrix: {model_name.replace(\"_\", \" \").title()}')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/confusion_matrix_{model_name}.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            print(f\"  Saved: {output_dir}/confusion_matrix_{model_name}.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. ROC Curve\n",
    "            if len(np.unique(self.y_test)) == 2:\n",
    "                fpr, tpr, thresholds = roc_curve(self.y_test, results['y_proba_test'])\n",
    "                auc = roc_auc_score(self.y_test, results['y_proba_test'])\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})', linewidth=2)\n",
    "                plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate (Recall)')\n",
    "                plt.title(f'ROC Curve: {model_name.replace(\"_\", \" \").title()}')\n",
    "                plt.legend()\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_dir}/roc_curve_{model_name}.png', \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                print(f\"  Saved: {output_dir}/roc_curve_{model_name}.png\")\n",
    "                plt.close()\n",
    "            \n",
    "            # 3. Prediction distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(results['y_proba_test'][self.y_test == 0], \n",
    "                    bins=30, alpha=0.6, label='True Low Risk', color='blue')\n",
    "            plt.hist(results['y_proba_test'][self.y_test == 1], \n",
    "                    bins=30, alpha=0.6, label='True High Risk', color='red')\n",
    "            plt.xlabel('Predicted Risk Probability')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(f'Prediction Distribution: {model_name.replace(\"_\", \" \").title()}')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/prediction_distribution_{model_name}.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            print(f\"  Saved: {output_dir}/prediction_distribution_{model_name}.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def save_models(self, output_dir='models'):\n",
    "        \"\"\"Save trained models to disk.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nSaving trained models...\")\n",
    "        for model_name, model in self.models.items():\n",
    "            filepath = f'{output_dir}/{model_name}.pkl'\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"  Saved: {filepath}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute full risk inference pipeline.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RISK INFERENCE MODEL PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # LOAD YOUR DATA HERE\n",
    "    # This should be the output from script 02 (with risk features)\n",
    "    data_path = 'data/processed/reports_with_features.csv'\n",
    "    \n",
    "    print(f\"\\nLoading data from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    # Initialize model with dataframe\n",
    "    risk_model = RiskInferenceModel(df)\n",
    "    \n",
    "    # Prepare features\n",
    "    risk_model.prepare_features()\n",
    "    \n",
    "    # Train models\n",
    "    risk_model.train_logistic_regression()\n",
    "    risk_model.train_random_forest()\n",
    "    \n",
    "    # Evaluate models\n",
    "    risk_model.evaluate_model('logistic_regression')\n",
    "    risk_model.evaluate_model('random_forest')\n",
    "    \n",
    "    # Generate visualizations\n",
    "    risk_model.visualize_results()\n",
    "    \n",
    "    # Save models\n",
    "    risk_model.save_models()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MODELING COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"  - High overall accuracy due to class imbalance\")\n",
    "    print(\"  - Low recall for minority risk class (expected)\")\n",
    "    print(\"  - Results demonstrate policy-relevant tradeoffs\")\n",
    "    print(\"\\nKey Outputs:\")\n",
    "    print(\"  - Trained models: models/\")\n",
    "    print(\"  - Evaluation plots: figures/\")\n",
    "    print(\"\\nNext Step: Run 04_policy_comparison.py\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d063d-90c2-48fe-b67b-16bf88a50d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
