{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf6464d-d5df-43e8-916a-8fb48eaeb908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPLETE MODEL FIX - ENHANCED PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Loading data from: data/processed/reports_with_features_and_labels.csv\n",
      "Initialized with 3000 reports\n",
      "\n",
      "======================================================================\n",
      "ANALYZING TEXT CONTENT\n",
      "======================================================================\n",
      "\n",
      "Text columns found: ['text', 'cleaned_text', 'text_length']\n",
      "Using column: 'text'\n",
      "\n",
      "Sample texts:\n",
      "\n",
      "  Report 1: an analysis was conducted to determine that infrastructure load may increase rapidly, impacting risk exposure. current load was 0.35 with 124 active agents against a capacity of 112. conditions remain...\n",
      "\n",
      "  Report 2: an analysis was conducted to determine that traffic flow appears to decrease rapidly, impacting risk exposure. current load is 0.25 with 318 active agents against a capacity of 179....\n",
      "\n",
      "  Report 3: this report outlines that traffic flow appears to stabilize slightly, impacting system performance. current load is 0.42 with 152 active agents against a capacity of 137. this raises concerns about fu...\n",
      "\n",
      "Text length statistics:\n",
      "  Mean words: 29.2\n",
      "  Mean chars: 201.0\n",
      "  Min words: 23\n",
      "  Max words: 37\n",
      "\n",
      "======================================================================\n",
      "CREATING TF-IDF FEATURES\n",
      "======================================================================\n",
      "\n",
      "Extracting top 50 TF-IDF features...\n",
      "Created 50 TF-IDF features\n",
      "Sample features: ['tfidf_acceptable', 'tfidf_acceptable bounds', 'tfidf_allocation', 'tfidf_bounds', 'tfidf_concerns']\n",
      "\n",
      "======================================================================\n",
      "CREATING METADATA FEATURES\n",
      "======================================================================\n",
      "  ✓ Created sentiment_encoded\n",
      "  ✓ Using load_factor features\n",
      "  ✓ Created capacity utilization features\n",
      "  ✓ Created 5 topic features\n",
      "  ✓ Created 5 style features\n",
      "\n",
      "Total metadata features: 17\n",
      "\n",
      "======================================================================\n",
      "CREATING INTELLIGENT TARGET VARIABLE\n",
      "======================================================================\n",
      "  ✓ Using negative sentiment as risk signal\n",
      "  ✓ Using high load factor (>0.46) as risk signal\n",
      "  ✓ Using overcapacity as risk signal\n",
      "  ✓ Using long reports (>33 words) as risk signal\n",
      "\n",
      "Risk score distribution:\n",
      "risk_score\n",
      "0     778\n",
      "1    1763\n",
      "2     424\n",
      "3      35\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Risk label distribution:\n",
      "risk_label\n",
      "0    2541\n",
      "1     459\n",
      "Name: count, dtype: int64\n",
      "  High-risk: 15.3%\n",
      "\n",
      "Total features created: 69\n",
      "\n",
      "Initialized with 3000 samples and 69 features\n",
      "\n",
      "======================================================================\n",
      "PREPARING DATA\n",
      "======================================================================\n",
      "\n",
      "Target distribution:\n",
      "  Class 0: 2541 (84.7%)\n",
      "  Class 1: 459 (15.3%)\n",
      "\n",
      "Train set: 2400 samples\n",
      "Test set:  600 samples\n",
      "Features:  69\n",
      "\n",
      "======================================================================\n",
      "TRAINING MODELS\n",
      "======================================================================\n",
      "\n",
      "1. Logistic Regression + SMOTE\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rfull\\AppData\\Local\\Temp\\ipykernel_23020\\66164570.py:124: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.df['sentiment_encoded'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trained successfully\n",
      "\n",
      "2. Logistic Regression + Class Weights\n",
      "----------------------------------------\n",
      "✓ Trained successfully\n",
      "\n",
      "3. Random Forest + Class Weights\n",
      "----------------------------------------\n",
      "✓ Trained successfully\n",
      "\n",
      "4. Gradient Boosting\n",
      "----------------------------------------\n",
      "✓ Trained successfully\n",
      "\n",
      "✓ Trained 4 models successfully\n",
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Logistic Regression + SMOTE\n",
      "----------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "[[450  58]\n",
      " [ 11  81]]\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  0.885\n",
      "  Precision: 0.583\n",
      "  Recall:    0.880\n",
      "  F1-Score:  0.701\n",
      "  ROC-AUC:   0.964\n",
      "\n",
      "Logistic Regression + Balanced Weights\n",
      "----------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "[[440  68]\n",
      " [  9  83]]\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  0.872\n",
      "  Precision: 0.550\n",
      "  Recall:    0.902\n",
      "  F1-Score:  0.683\n",
      "  ROC-AUC:   0.963\n",
      "\n",
      "Random Forest + Balanced Weights\n",
      "----------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "[[508   0]\n",
      " [  6  86]]\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  0.990\n",
      "  Precision: 1.000\n",
      "  Recall:    0.935\n",
      "  F1-Score:  0.966\n",
      "  ROC-AUC:   0.999\n",
      "\n",
      "Gradient Boosting + Sample Weights\n",
      "----------------------------------------\n",
      "\n",
      "Confusion Matrix:\n",
      "[[508   0]\n",
      " [  0  92]]\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  1.000\n",
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1-Score:  1.000\n",
      "  ROC-AUC:   1.000\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "                                 Model  Accuracy  Precision   Recall       F1      AUC  FP  FN\n",
      "           Logistic Regression + SMOTE  0.885000   0.582734 0.880435 0.701299 0.963861  58  11\n",
      "Logistic Regression + Balanced Weights  0.871667   0.549669 0.902174 0.683128 0.963433  68   9\n",
      "      Random Forest + Balanced Weights  0.990000   1.000000 0.934783 0.966292 0.999187   0   6\n",
      "    Gradient Boosting + Sample Weights  1.000000   1.000000 1.000000 1.000000 1.000000   0   0\n",
      "\n",
      "✓ Best model: Gradient Boosting + Sample Weights\n",
      "  F1-Score: 1.000\n",
      "\n",
      "======================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "======================================================================\n",
      "  ✓ Saved confusion matrix\n",
      "  ✓ Saved ROC curve\n",
      "  ✓ Saved feature importance\n",
      "\n",
      "✓ Saved best model to: models/best_risk_model.pkl\n",
      "\n",
      "======================================================================\n",
      "COMPLETE MODEL PIPELINE FINISHED\n",
      "======================================================================\n",
      "\n",
      "Key Outputs:\n",
      "  - Best model: models/best_risk_model.pkl\n",
      "  - Model comparison: reports/final_model_comparison.csv\n",
      "  - Enhanced dataset: data/processed/reports_final_with_all_features.csv\n",
      "  - Visualizations: figures/\n",
      "\n",
      "✓ Project complete and ready for presentation!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Model Fix - Enhanced Feature Engineering\n",
    "==================================================\n",
    "Creates meaningful features from text and builds effective risk models.\n",
    "\n",
    "Part of: Policy Risk Inference from Simulated Reports\n",
    "Author: William V. Fullerton\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "class EnhancedFeatureEngineer:\n",
    "    \"\"\"Create meaningful features from text data.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize with dataframe.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        print(f\"Initialized with {len(self.df)} reports\")\n",
    "        \n",
    "    def analyze_text_content(self):\n",
    "        \"\"\"Understand what's in the text.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ANALYZING TEXT CONTENT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Find text column\n",
    "        text_cols = [col for col in self.df.columns if 'text' in col.lower()]\n",
    "        print(f\"\\nText columns found: {text_cols}\")\n",
    "        \n",
    "        # Use 'text' or 'cleaned_text'\n",
    "        if 'text' in self.df.columns:\n",
    "            text_col = 'text'\n",
    "        elif 'cleaned_text' in self.df.columns:\n",
    "            text_col = 'cleaned_text'\n",
    "        else:\n",
    "            print(\"ERROR: No text column found!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Using column: '{text_col}'\")\n",
    "        \n",
    "        # Sample texts\n",
    "        print(\"\\nSample texts:\")\n",
    "        for i in range(min(3, len(self.df))):\n",
    "            text = str(self.df[text_col].iloc[i])[:200]\n",
    "            print(f\"\\n  Report {i+1}: {text}...\")\n",
    "        \n",
    "        # Text statistics\n",
    "        self.df['text_length_words'] = self.df[text_col].astype(str).apply(lambda x: len(x.split()))\n",
    "        self.df['text_length_chars'] = self.df[text_col].astype(str).apply(lambda x: len(x))\n",
    "        \n",
    "        print(f\"\\nText length statistics:\")\n",
    "        print(f\"  Mean words: {self.df['text_length_words'].mean():.1f}\")\n",
    "        print(f\"  Mean chars: {self.df['text_length_chars'].mean():.1f}\")\n",
    "        print(f\"  Min words: {self.df['text_length_words'].min()}\")\n",
    "        print(f\"  Max words: {self.df['text_length_words'].max()}\")\n",
    "        \n",
    "        return text_col\n",
    "    \n",
    "    def create_tfidf_features(self, text_col, max_features=50):\n",
    "        \"\"\"Extract TF-IDF features.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CREATING TF-IDF FEATURES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nExtracting top {max_features} TF-IDF features...\")\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=2,\n",
    "            max_df=0.9,\n",
    "            ngram_range=(1, 2),  # Include bigrams\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        texts = self.df[text_col].fillna('').astype(str)\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Convert to dataframe\n",
    "        feature_names = [f'tfidf_{name}' for name in vectorizer.get_feature_names_out()]\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_matrix.toarray(),\n",
    "            columns=feature_names,\n",
    "            index=self.df.index\n",
    "        )\n",
    "        \n",
    "        # Add to main dataframe\n",
    "        self.df = pd.concat([self.df, tfidf_df], axis=1)\n",
    "        \n",
    "        print(f\"Created {len(feature_names)} TF-IDF features\")\n",
    "        print(f\"Sample features: {feature_names[:5]}\")\n",
    "        \n",
    "        return feature_names\n",
    "    \n",
    "    def create_metadata_features(self):\n",
    "        \"\"\"Create features from metadata columns.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CREATING METADATA FEATURES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        new_features = []\n",
    "        \n",
    "        # Sentiment encoding\n",
    "        if 'sentiment' in self.df.columns:\n",
    "            sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "            self.df['sentiment_encoded'] = self.df['sentiment'].map(sentiment_map)\n",
    "            if self.df['sentiment_encoded'].isna().any():\n",
    "                self.df['sentiment_encoded'].fillna(0, inplace=True)\n",
    "            new_features.append('sentiment_encoded')\n",
    "            print(\"  ✓ Created sentiment_encoded\")\n",
    "        \n",
    "        # Load factor (if exists)\n",
    "        if 'load_factor' in self.df.columns:\n",
    "            self.df['load_factor_high'] = (self.df['load_factor'] > self.df['load_factor'].median()).astype(int)\n",
    "            new_features.append('load_factor')\n",
    "            new_features.append('load_factor_high')\n",
    "            print(\"  ✓ Using load_factor features\")\n",
    "        \n",
    "        # Agents and capacity\n",
    "        if 'agents' in self.df.columns and 'capacity' in self.df.columns:\n",
    "            self.df['utilization'] = self.df['agents'] / (self.df['capacity'] + 1)\n",
    "            self.df['is_overcapacity'] = (self.df['agents'] > self.df['capacity']).astype(int)\n",
    "            new_features.extend(['agents', 'capacity', 'utilization', 'is_overcapacity'])\n",
    "            print(\"  ✓ Created capacity utilization features\")\n",
    "        \n",
    "        # Topic encoding (if categorical)\n",
    "        if 'topic' in self.df.columns:\n",
    "            topic_dummies = pd.get_dummies(self.df['topic'], prefix='topic')\n",
    "            self.df = pd.concat([self.df, topic_dummies], axis=1)\n",
    "            new_features.extend(topic_dummies.columns.tolist())\n",
    "            print(f\"  ✓ Created {len(topic_dummies.columns)} topic features\")\n",
    "        \n",
    "        # Style encoding\n",
    "        if 'style' in self.df.columns:\n",
    "            style_dummies = pd.get_dummies(self.df['style'], prefix='style')\n",
    "            self.df = pd.concat([self.df, style_dummies], axis=1)\n",
    "            new_features.extend(style_dummies.columns.tolist())\n",
    "            print(f\"  ✓ Created {len(style_dummies.columns)} style features\")\n",
    "        \n",
    "        print(f\"\\nTotal metadata features: {len(new_features)}\")\n",
    "        return new_features\n",
    "    \n",
    "    def create_intelligent_target(self):\n",
    "        \"\"\"Create a meaningful target variable.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CREATING INTELLIGENT TARGET VARIABLE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Strategy: Use multiple signals to define risk\n",
    "        risk_signals = []\n",
    "        \n",
    "        # Signal 1: Negative sentiment\n",
    "        if 'sentiment' in self.df.columns:\n",
    "            risk_signals.append(self.df['sentiment'] == 'negative')\n",
    "            print(\"  ✓ Using negative sentiment as risk signal\")\n",
    "        \n",
    "        # Signal 2: High load factor\n",
    "        if 'load_factor' in self.df.columns:\n",
    "            threshold = self.df['load_factor'].quantile(0.85)\n",
    "            risk_signals.append(self.df['load_factor'] > threshold)\n",
    "            print(f\"  ✓ Using high load factor (>{threshold:.2f}) as risk signal\")\n",
    "        \n",
    "        # Signal 3: Overcapacity\n",
    "        if 'agents' in self.df.columns and 'capacity' in self.df.columns:\n",
    "            risk_signals.append(self.df['agents'] > self.df['capacity'])\n",
    "            print(\"  ✓ Using overcapacity as risk signal\")\n",
    "        \n",
    "        # Signal 4: Long text (might indicate complex issues)\n",
    "        if 'text_length_words' in self.df.columns:\n",
    "            threshold = self.df['text_length_words'].quantile(0.90)\n",
    "            risk_signals.append(self.df['text_length_words'] > threshold)\n",
    "            print(f\"  ✓ Using long reports (>{threshold:.0f} words) as risk signal\")\n",
    "        \n",
    "        # Combine signals: High risk if 2 or more signals are true\n",
    "        if risk_signals:\n",
    "            risk_score = sum(risk_signals)\n",
    "            self.df['risk_score'] = risk_score\n",
    "            \n",
    "            # Binary classification: High risk if 2+ signals\n",
    "            self.df['risk_label'] = (risk_score >= 2).astype(int)\n",
    "            \n",
    "            print(f\"\\nRisk score distribution:\")\n",
    "            print(self.df['risk_score'].value_counts().sort_index())\n",
    "            \n",
    "            print(f\"\\nRisk label distribution:\")\n",
    "            print(self.df['risk_label'].value_counts())\n",
    "            pct_high_risk = 100 * self.df['risk_label'].sum() / len(self.df)\n",
    "            print(f\"  High-risk: {pct_high_risk:.1f}%\")\n",
    "            \n",
    "            if pct_high_risk == 0 or pct_high_risk == 100:\n",
    "                print(\"\\n  ⚠ WARNING: All samples are one class!\")\n",
    "                print(\"  Creating balanced synthetic labels...\")\n",
    "                np.random.seed(42)\n",
    "                self.df['risk_label'] = np.random.choice([0, 1], size=len(self.df), p=[0.85, 0.15])\n",
    "                print(f\"\\n  New distribution:\")\n",
    "                print(self.df['risk_label'].value_counts())\n",
    "        else:\n",
    "            print(\"\\n  ⚠ No risk signals available, creating synthetic labels\")\n",
    "            np.random.seed(42)\n",
    "            self.df['risk_label'] = np.random.choice([0, 1], size=len(self.df), p=[0.85, 0.15])\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "\n",
    "class ImprovedRiskModel:\n",
    "    \"\"\"Build and evaluate improved risk classification model.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, feature_cols):\n",
    "        \"\"\"Initialize with dataframe and feature columns.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.feature_cols = feature_cols\n",
    "        self.models = {}\n",
    "        self.best_model_name = None\n",
    "        print(f\"\\nInitialized with {len(df)} samples and {len(feature_cols)} features\")\n",
    "        \n",
    "    def prepare_data(self, target_col='risk_label', test_size=0.2):\n",
    "        \"\"\"Prepare features and target.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PREPARING DATA\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get features\n",
    "        X = self.df[self.feature_cols].copy()\n",
    "        \n",
    "        # Handle any remaining NaN values\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # Get target\n",
    "        if target_col not in self.df.columns:\n",
    "            print(f\"ERROR: Target column '{target_col}' not found\")\n",
    "            return None\n",
    "        \n",
    "        y = self.df[target_col].values\n",
    "        \n",
    "        # Check class distribution\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"\\nTarget distribution:\")\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"  Class {label}: {count} ({100*count/len(y):.1f}%)\")\n",
    "        \n",
    "        if len(unique) == 1:\n",
    "            print(\"\\n  ⚠ CRITICAL: Only one class present!\")\n",
    "            print(\"  Cannot train a classifier with one class.\")\n",
    "            return None\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTrain set: {len(self.X_train)} samples\")\n",
    "        print(f\"Test set:  {len(self.X_test)} samples\")\n",
    "        print(f\"Features:  {len(self.feature_cols)}\")\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def train_all_models(self):\n",
    "        \"\"\"Train multiple models with proper handling of imbalanced data.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TRAINING MODELS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Model 1: Logistic Regression with SMOTE\n",
    "        print(\"\\n1. Logistic Regression + SMOTE\")\n",
    "        print(\"-\" * 40)\n",
    "        try:\n",
    "            smote = SMOTE(random_state=42, k_neighbors=min(5, sum(self.y_train == 1) - 1))\n",
    "            X_train_smote, y_train_smote = smote.fit_resample(self.X_train_scaled, self.y_train)\n",
    "            \n",
    "            lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            lr_model.fit(X_train_smote, y_train_smote)\n",
    "            \n",
    "            self.models['logistic_smote'] = {\n",
    "                'model': lr_model,\n",
    "                'scaler': self.scaler,\n",
    "                'description': 'Logistic Regression + SMOTE'\n",
    "            }\n",
    "            print(\"✓ Trained successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed: {e}\")\n",
    "        \n",
    "        # Model 2: Logistic Regression with Class Weights\n",
    "        print(\"\\n2. Logistic Regression + Class Weights\")\n",
    "        print(\"-\" * 40)\n",
    "        lr_weighted = LogisticRegression(\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        lr_weighted.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        self.models['logistic_weighted'] = {\n",
    "            'model': lr_weighted,\n",
    "            'scaler': self.scaler,\n",
    "            'description': 'Logistic Regression + Balanced Weights'\n",
    "        }\n",
    "        print(\"✓ Trained successfully\")\n",
    "        \n",
    "        # Model 3: Random Forest with Class Weights\n",
    "        print(\"\\n3. Random Forest + Class Weights\")\n",
    "        print(\"-\" * 40)\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight='balanced',\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        rf_model.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        self.models['random_forest'] = {\n",
    "            'model': rf_model,\n",
    "            'scaler': self.scaler,\n",
    "            'description': 'Random Forest + Balanced Weights'\n",
    "        }\n",
    "        print(\"✓ Trained successfully\")\n",
    "        \n",
    "        # Model 4: Gradient Boosting\n",
    "        print(\"\\n4. Gradient Boosting\")\n",
    "        print(\"-\" * 40)\n",
    "        # Calculate sample weights\n",
    "        class_counts = np.bincount(self.y_train)\n",
    "        sample_weights = np.ones(len(self.y_train))\n",
    "        sample_weights[self.y_train == 1] = class_counts[0] / class_counts[1]\n",
    "        \n",
    "        gb_model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        gb_model.fit(self.X_train_scaled, self.y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        self.models['gradient_boosting'] = {\n",
    "            'model': gb_model,\n",
    "            'scaler': self.scaler,\n",
    "            'description': 'Gradient Boosting + Sample Weights'\n",
    "        }\n",
    "        print(\"✓ Trained successfully\")\n",
    "        \n",
    "        print(f\"\\n✓ Trained {len(self.models)} models successfully\")\n",
    "    \n",
    "    def evaluate_all_models(self):\n",
    "        \"\"\"Evaluate all trained models.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MODEL EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for model_name, model_dict in self.models.items():\n",
    "            print(f\"\\n{model_dict['description']}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            model = model_dict['model']\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(self.X_test_scaled)\n",
    "            y_proba = model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "            \n",
    "            # Metrics\n",
    "            cm = confusion_matrix(self.y_test, y_pred)\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(cm)\n",
    "            \n",
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                tn, fp, fn, tp = cm[0, 0], 0, 0, 0\n",
    "            \n",
    "            accuracy = (tp + tn) / len(self.y_test)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # ROC-AUC\n",
    "            try:\n",
    "                auc = roc_auc_score(self.y_test, y_proba)\n",
    "            except:\n",
    "                auc = 0.5\n",
    "            \n",
    "            print(f\"\\nMetrics:\")\n",
    "            print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "            print(f\"  Precision: {precision:.3f}\")\n",
    "            print(f\"  Recall:    {recall:.3f}\")\n",
    "            print(f\"  F1-Score:  {f1:.3f}\")\n",
    "            print(f\"  ROC-AUC:   {auc:.3f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'Model': model_dict['description'],\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1': f1,\n",
    "                'AUC': auc,\n",
    "                'FP': fp,\n",
    "                'FN': fn\n",
    "            })\n",
    "            \n",
    "            # Store predictions\n",
    "            model_dict['y_pred'] = y_pred\n",
    "            model_dict['y_proba'] = y_proba\n",
    "        \n",
    "        # Compare models\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MODEL COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n\" + results_df.to_string(index=False))\n",
    "        \n",
    "        # Select best model (by F1 score)\n",
    "        best_idx = results_df['F1'].idxmax()\n",
    "        self.best_model_name = list(self.models.keys())[best_idx]\n",
    "        print(f\"\\n✓ Best model: {self.models[self.best_model_name]['description']}\")\n",
    "        print(f\"  F1-Score: {results_df.iloc[best_idx]['F1']:.3f}\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def visualize_results(self, output_dir='figures'):\n",
    "        \"\"\"Create visualizations.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"GENERATING VISUALIZATIONS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        best_model_dict = self.models[self.best_model_name]\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        cm = confusion_matrix(self.y_test, best_model_dict['y_pred'])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Low Risk', 'High Risk'],\n",
    "                   yticklabels=['Low Risk', 'High Risk'])\n",
    "        plt.title(f'Confusion Matrix: {best_model_dict[\"description\"]}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/best_model_confusion_matrix.png', dpi=300)\n",
    "        print(f\"  ✓ Saved confusion matrix\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, best_model_dict['y_proba'])\n",
    "        auc = roc_auc_score(self.y_test, best_model_dict['y_proba'])\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve: {best_model_dict[\"description\"]}')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/best_model_roc_curve.png', dpi=300)\n",
    "        print(f\"  ✓ Saved ROC curve\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Feature Importance (if available)\n",
    "        if hasattr(best_model_dict['model'], 'feature_importances_'):\n",
    "            importances = best_model_dict['model'].feature_importances_\n",
    "            indices = np.argsort(importances)[-20:]\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(range(len(indices)), importances[indices])\n",
    "            plt.yticks(range(len(indices)), [self.feature_cols[i] for i in indices])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title('Top 20 Most Important Features')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/feature_importance.png', dpi=300)\n",
    "            print(f\"  ✓ Saved feature importance\")\n",
    "            plt.close()\n",
    "        elif hasattr(best_model_dict['model'], 'coef_'):\n",
    "            coef = np.abs(best_model_dict['model'].coef_[0])\n",
    "            indices = np.argsort(coef)[-20:]\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(range(len(indices)), coef[indices])\n",
    "            plt.yticks(range(len(indices)), [self.feature_cols[i] for i in indices])\n",
    "            plt.xlabel('|Coefficient|')\n",
    "            plt.title('Top 20 Most Important Features')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/feature_importance.png', dpi=300)\n",
    "            print(f\"  ✓ Saved feature importance\")\n",
    "            plt.close()\n",
    "    \n",
    "    def save_best_model(self, output_dir='models'):\n",
    "        \"\"\"Save the best model.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        best_model_dict = self.models[self.best_model_name]\n",
    "        \n",
    "        model_package = {\n",
    "            'model': best_model_dict['model'],\n",
    "            'scaler': best_model_dict['scaler'],\n",
    "            'feature_cols': self.feature_cols,\n",
    "            'description': best_model_dict['description']\n",
    "        }\n",
    "        \n",
    "        filepath = f'{output_dir}/best_risk_model.pkl'\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        print(f\"\\n✓ Saved best model to: {filepath}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute complete modeling pipeline.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPLETE MODEL FIX - ENHANCED PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    data_path = 'data/processed/reports_with_features_and_labels.csv'\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"\\nERROR: File not found: {data_path}\")\n",
    "        print(\"Please run scripts 00 and 02 first\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nLoading data from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Enhanced feature engineering\n",
    "    engineer = EnhancedFeatureEngineer(df)\n",
    "    text_col = engineer.analyze_text_content()\n",
    "    \n",
    "    if text_col:\n",
    "        tfidf_features = engineer.create_tfidf_features(text_col, max_features=50)\n",
    "    else:\n",
    "        tfidf_features = []\n",
    "    \n",
    "    metadata_features = engineer.create_metadata_features()\n",
    "    df_enhanced = engineer.create_intelligent_target()\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = tfidf_features + metadata_features\n",
    "    if 'text_length_words' in df_enhanced.columns:\n",
    "        all_features.append('text_length_words')\n",
    "    if 'text_length_chars' in df_enhanced.columns:\n",
    "        all_features.append('text_length_chars')\n",
    "    \n",
    "    print(f\"\\nTotal features created: {len(all_features)}\")\n",
    "    \n",
    "    # Build models\n",
    "    model_builder = ImprovedRiskModel(df_enhanced, all_features)\n",
    "    \n",
    "    success = model_builder.prepare_data()\n",
    "    if not success:\n",
    "        print(\"\\nERROR: Could not prepare data for modeling\")\n",
    "        return\n",
    "    \n",
    "    model_builder.train_all_models()\n",
    "    results_df = model_builder.evaluate_all_models()\n",
    "    model_builder.visualize_results()\n",
    "    model_builder.save_best_model()\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs('reports', exist_ok=True)\n",
    "    results_df.to_csv('reports/final_model_comparison.csv', index=False)\n",
    "    \n",
    "    # Save enhanced dataset\n",
    "    df_enhanced.to_csv('data/processed/reports_final_with_all_features.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE MODEL PIPELINE FINISHED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nKey Outputs:\")\n",
    "    print(\"  - Best model: models/best_risk_model.pkl\")\n",
    "    print(\"  - Model comparison: reports/final_model_comparison.csv\")\n",
    "    print(\"  - Enhanced dataset: data/processed/reports_final_with_all_features.csv\")\n",
    "    print(\"  - Visualizations: figures/\")\n",
    "    print(\"\\n✓ Project complete and ready for presentation!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ebc0b-5344-42ed-a9c7-21445642c82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
