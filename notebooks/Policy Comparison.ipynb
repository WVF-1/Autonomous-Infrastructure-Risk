{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68215170-581f-4355-bd58-daf2c8ca4825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: C:\\Users\\rfull\\Building Data Together Weeklies\\Autonomous Infrastructure Risk\\data\\processed\\reports_with_features_and_labels.csv\n",
      "======================================================================\n",
      "POLICY COMPARISON & INTERPRETATION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Loading data from: data/processed/reports_with_features.csv\n",
      "Loaded 3000 rows\n",
      "\n",
      "Loading models from: models\n",
      "  Loaded: logistic_regression\n",
      "  Loaded: random_forest\n",
      "Initialized with 3000 reports and 2 models\n",
      "\n",
      "Defining policy scenarios...\n",
      "  Conservative: threshold=0.3, Safety over efficiency\n",
      "  Balanced: threshold=0.5, Equal weight to both error types\n",
      "  Aggressive: threshold=0.7, Efficiency over caution\n",
      "\n",
      "======================================================================\n",
      "POLICY SCENARIO SIMULATION: LOGISTIC_REGRESSION\n",
      "======================================================================\n",
      "Note: Using simulated labels (98% low-risk, 2% high-risk)\n",
      "\n",
      "CONSERVATIVE Policy (threshold=0.3)\n",
      "  Low threshold - flag more potential risks (minimize false negatives)\n",
      "  Priority: Safety over efficiency\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN=   0  FP=2943\n",
      "    FN=   0  TP=  57\n",
      "\n",
      "  Performance:\n",
      "    Accuracy:  0.019\n",
      "    Precision: 0.019\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.037\n",
      "    FP Rate:   1.000\n",
      "\n",
      "  Policy Implications:\n",
      "    ⚠ 2943 false alarms (wasted review resources)\n",
      "\n",
      "  Decision Context:\n",
      "    → More false alarms, but catches more true risks\n",
      "    → Best when cost of missing risk >> cost of false alarm\n",
      "\n",
      "BALANCED Policy (threshold=0.5)\n",
      "  Standard threshold - balance precision and recall\n",
      "  Priority: Equal weight to both error types\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN=   0  FP=2943\n",
      "    FN=   0  TP=  57\n",
      "\n",
      "  Performance:\n",
      "    Accuracy:  0.019\n",
      "    Precision: 0.019\n",
      "    Recall:    1.000\n",
      "    F1-Score:  0.037\n",
      "    FP Rate:   1.000\n",
      "\n",
      "  Policy Implications:\n",
      "    ⚠ 2943 false alarms (wasted review resources)\n",
      "\n",
      "  Decision Context:\n",
      "    → Moderate tradeoff between precision and recall\n",
      "    → Standard approach for general use\n",
      "\n",
      "AGGRESSIVE Policy (threshold=0.7)\n",
      "  High threshold - reduce false alarms (minimize false positives)\n",
      "  Priority: Efficiency over caution\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN=2943  FP=   0\n",
      "    FN=  57  TP=   0\n",
      "\n",
      "  Performance:\n",
      "    Accuracy:  0.981\n",
      "    Precision: 0.000\n",
      "    Recall:    0.000\n",
      "    F1-Score:  0.000\n",
      "    FP Rate:   0.000\n",
      "\n",
      "  Policy Implications:\n",
      "    ⚠ 57 high-risk cases missed (could lead to incidents)\n",
      "\n",
      "  Decision Context:\n",
      "    → Fewer false alarms, but may miss some risks\n",
      "    → Best when review capacity is constrained\n",
      "\n",
      "======================================================================\n",
      "COMPARATIVE POLICY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Scenario Comparison Table:\n",
      "    Scenario  Threshold  Accuracy  Precision  Recall  F1-Score  False Positives  False Negatives  FP Rate\n",
      "Conservative        0.3     0.019      0.019     1.0  0.037291             2943                0      1.0\n",
      "    Balanced        0.5     0.019      0.019     1.0  0.037291             2943                0      1.0\n",
      "  Aggressive        0.7     0.981      0.000     0.0  0.000000                0               57      0.0\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS\n",
      "======================================================================\n",
      "\n",
      "1. Accuracy vs Minority Class Performance:\n",
      "   All scenarios show high accuracy (~98%) due to class imbalance.\n",
      "   However, recall for minority class varies significantly.\n",
      "\n",
      "2. False Negative vs False Positive Tradeoff:\n",
      "   Conservative policy: Higher recall (1.000), more false alarms\n",
      "   Aggressive policy: Higher precision (0.019), more missed risks\n",
      "\n",
      "3. Policy Recommendation:\n",
      "   Choice depends on institutional risk tolerance:\n",
      "   • High-stakes domains → Conservative threshold\n",
      "   • Resource-constrained → Aggressive threshold\n",
      "   • Standard operations → Balanced threshold\n",
      "\n",
      "Generating policy comparison visualizations...\n",
      "  Saved: figures/policy_metrics_comparison.png\n",
      "  Saved: figures/error_types_comparison.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rfull\\AppData\\Local\\Temp\\ipykernel_9248\\1095819669.py:332: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: figures/threshold_sensitivity.png\n",
      "\n",
      "Generating policy comparison report...\n",
      "  Saved: reports/policy_comparison_report.txt\n",
      "\n",
      "======================================================================\n",
      "POLICY ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Key Outputs:\n",
      "  - Comparison visualizations: figures/\n",
      "  - Policy report: reports/policy_comparison_report.txt\n",
      "\n",
      "Next Steps:\n",
      "  - Review threshold sensitivity analysis\n",
      "  - Consider cost-sensitive learning approaches\n",
      "  - Implement human-in-the-loop review for borderline cases\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Policy Comparison & Interpretation Script\n",
    "==========================================\n",
    "Compares risk signals across simulated policy regimes, examines \n",
    "false negatives vs false positives, and frames results in a \n",
    "decision-making context.\n",
    "\n",
    "Part of: Policy Risk Inference from Simulated Reports\n",
    "Author: William V. Fullerton\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data/processed/reports_with_features_and_labels.csv\")\n",
    "\n",
    "print(f\"\\nLoading data from: {DATA_PATH.resolve()}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "class PolicyComparator:\n",
    "    \"\"\"Compare model performance across policy scenarios.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, models):\n",
    "        \"\"\"Initialize comparator with dataframe and models.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.models = models\n",
    "        self.policy_scenarios = {}\n",
    "        print(f\"Initialized with {len(self.df)} reports and {len(self.models)} models\")\n",
    "        \n",
    "    def define_policy_scenarios(self):\n",
    "        \"\"\"Define different policy decision thresholds.\"\"\"\n",
    "        print(\"\\nDefining policy scenarios...\")\n",
    "        \n",
    "        self.policy_scenarios = {\n",
    "            'conservative': {\n",
    "                'threshold': 0.3,\n",
    "                'description': 'Low threshold - flag more potential risks (minimize false negatives)',\n",
    "                'priority': 'Safety over efficiency'\n",
    "            },\n",
    "            'balanced': {\n",
    "                'threshold': 0.5,\n",
    "                'description': 'Standard threshold - balance precision and recall',\n",
    "                'priority': 'Equal weight to both error types'\n",
    "            },\n",
    "            'aggressive': {\n",
    "                'threshold': 0.7,\n",
    "                'description': 'High threshold - reduce false alarms (minimize false positives)',\n",
    "                'priority': 'Efficiency over caution'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for name, scenario in self.policy_scenarios.items():\n",
    "            print(f\"  {name.title()}: threshold={scenario['threshold']}, {scenario['priority']}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def simulate_policy_decisions(self, model_name='logistic_regression'):\n",
    "        \"\"\"Simulate decisions under different policy thresholds.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"POLICY SCENARIO SIMULATION: {model_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Prepare features (assuming same features used in training)\n",
    "        risk_feature_cols = [col for col in self.df.columns if col.startswith('risk_')]\n",
    "        \n",
    "        if not risk_feature_cols:\n",
    "            print(\"Warning: Using dummy features for demonstration\")\n",
    "            X = np.random.randn(len(self.df), 2)\n",
    "        else:\n",
    "            X = self.df[risk_feature_cols].values\n",
    "        \n",
    "        # Get true labels (or simulate)\n",
    "        label_cols = [col for col in self.df.columns if 'label' in col.lower() or 'target' in col.lower()]\n",
    "        if label_cols:\n",
    "            y_true = self.df[label_cols[0]].values\n",
    "            print(f\"Using target column: {label_cols[0]}\")\n",
    "        else:\n",
    "            print(\"Note: Using simulated labels (98% low-risk, 2% high-risk)\")\n",
    "            y_true = np.random.choice([0, 1], size=len(self.df), p=[0.98, 0.02])\n",
    "        \n",
    "        # Get model predictions\n",
    "        model = self.models.get(model_name)\n",
    "        if model is None:\n",
    "            print(f\"Warning: Model {model_name} not found. Available: {list(self.models.keys())}\")\n",
    "            return self\n",
    "        \n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Evaluate each policy scenario\n",
    "        results = {}\n",
    "        \n",
    "        for scenario_name, scenario in self.policy_scenarios.items():\n",
    "            threshold = scenario['threshold']\n",
    "            y_pred = (y_proba >= threshold).astype(int)\n",
    "            \n",
    "            # Calculate confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (cm[0,0], 0, 0, 0)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            total = len(y_true)\n",
    "            accuracy = (tp + tn) / total\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # False positive rate\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            \n",
    "            # Store results\n",
    "            results[scenario_name] = {\n",
    "                'threshold': threshold,\n",
    "                'predictions': y_pred,\n",
    "                'confusion_matrix': cm,\n",
    "                'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'fpr': fpr\n",
    "            }\n",
    "            \n",
    "            # Print scenario results\n",
    "            print(f\"\\n{scenario_name.upper()} Policy (threshold={threshold})\")\n",
    "            print(f\"  {scenario['description']}\")\n",
    "            print(f\"  Priority: {scenario['priority']}\")\n",
    "            print(f\"\\n  Confusion Matrix:\")\n",
    "            print(f\"    TN={tn:4d}  FP={fp:4d}\")\n",
    "            print(f\"    FN={fn:4d}  TP={tp:4d}\")\n",
    "            print(f\"\\n  Performance:\")\n",
    "            print(f\"    Accuracy:  {accuracy:.3f}\")\n",
    "            print(f\"    Precision: {precision:.3f}\")\n",
    "            print(f\"    Recall:    {recall:.3f}\")\n",
    "            print(f\"    F1-Score:  {f1:.3f}\")\n",
    "            print(f\"    FP Rate:   {fpr:.3f}\")\n",
    "            \n",
    "            # Policy interpretation\n",
    "            print(f\"\\n  Policy Implications:\")\n",
    "            if fn > 0:\n",
    "                print(f\"    ⚠ {fn} high-risk cases missed (could lead to incidents)\")\n",
    "            if fp > 0:\n",
    "                print(f\"    ⚠ {fp} false alarms (wasted review resources)\")\n",
    "            \n",
    "            # Cost-benefit framing\n",
    "            print(f\"\\n  Decision Context:\")\n",
    "            if scenario_name == 'conservative':\n",
    "                print(f\"    → More false alarms, but catches more true risks\")\n",
    "                print(f\"    → Best when cost of missing risk >> cost of false alarm\")\n",
    "            elif scenario_name == 'balanced':\n",
    "                print(f\"    → Moderate tradeoff between precision and recall\")\n",
    "                print(f\"    → Standard approach for general use\")\n",
    "            else:  # aggressive\n",
    "                print(f\"    → Fewer false alarms, but may miss some risks\")\n",
    "                print(f\"    → Best when review capacity is constrained\")\n",
    "        \n",
    "        # Store results for visualization\n",
    "        self.policy_results = {\n",
    "            'model_name': model_name,\n",
    "            'scenarios': results,\n",
    "            'y_true': y_true,\n",
    "            'y_proba': y_proba\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def compare_scenarios(self):\n",
    "        \"\"\"Generate comparative analysis across policy scenarios.\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"COMPARATIVE POLICY ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        results = self.policy_results['scenarios']\n",
    "        \n",
    "        # Create comparison dataframe\n",
    "        comparison_data = []\n",
    "        for scenario_name, metrics in results.items():\n",
    "            comparison_data.append({\n",
    "                'Scenario': scenario_name.title(),\n",
    "                'Threshold': metrics['threshold'],\n",
    "                'Accuracy': metrics['accuracy'],\n",
    "                'Precision': metrics['precision'],\n",
    "                'Recall': metrics['recall'],\n",
    "                'F1-Score': metrics['f1'],\n",
    "                'False Positives': metrics['fp'],\n",
    "                'False Negatives': metrics['fn'],\n",
    "                'FP Rate': metrics['fpr']\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        print(\"\\nScenario Comparison Table:\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Key insights\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"KEY INSIGHTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(\"\\n1. Accuracy vs Minority Class Performance:\")\n",
    "        print(\"   All scenarios show high accuracy (~98%) due to class imbalance.\")\n",
    "        print(\"   However, recall for minority class varies significantly.\")\n",
    "        \n",
    "        print(\"\\n2. False Negative vs False Positive Tradeoff:\")\n",
    "        best_recall = max(r['recall'] for r in results.values())\n",
    "        best_precision = max(r['precision'] for r in results.values())\n",
    "        print(f\"   Conservative policy: Higher recall ({best_recall:.3f}), more false alarms\")\n",
    "        print(f\"   Aggressive policy: Higher precision ({best_precision:.3f}), more missed risks\")\n",
    "        \n",
    "        print(\"\\n3. Policy Recommendation:\")\n",
    "        print(\"   Choice depends on institutional risk tolerance:\")\n",
    "        print(\"   • High-stakes domains → Conservative threshold\")\n",
    "        print(\"   • Resource-constrained → Aggressive threshold\")\n",
    "        print(\"   • Standard operations → Balanced threshold\")\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    def visualize_policy_comparison(self, output_dir='figures'):\n",
    "        \"\"\"Create visualizations comparing policy scenarios.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\nGenerating policy comparison visualizations...\")\n",
    "        \n",
    "        results = self.policy_results['scenarios']\n",
    "        \n",
    "        # 1. Metrics comparison\n",
    "        scenarios = list(results.keys())\n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, metric in enumerate(metrics_to_plot):\n",
    "            values = [results[s][metric] for s in scenarios]\n",
    "            axes[idx].bar(scenarios, values, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "            axes[idx].set_ylabel(metric.replace('_', ' ').title())\n",
    "            axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()} by Policy Scenario')\n",
    "            axes[idx].set_ylim([0, 1])\n",
    "            axes[idx].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/policy_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"  Saved: {output_dir}/policy_metrics_comparison.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Error type comparison\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        x = np.arange(len(scenarios))\n",
    "        width = 0.35\n",
    "        \n",
    "        fp_counts = [results[s]['fp'] for s in scenarios]\n",
    "        fn_counts = [results[s]['fn'] for s in scenarios]\n",
    "        \n",
    "        ax.bar(x - width/2, fp_counts, width, label='False Positives (False Alarms)', color='orange')\n",
    "        ax.bar(x + width/2, fn_counts, width, label='False Negatives (Missed Risks)', color='red')\n",
    "        \n",
    "        ax.set_xlabel('Policy Scenario')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Error Types by Policy Scenario')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([s.title() for s in scenarios])\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/error_types_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"  Saved: {output_dir}/error_types_comparison.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Threshold sensitivity\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        thresholds = np.linspace(0.1, 0.9, 50)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        y_true = self.policy_results['y_true']\n",
    "        y_proba = self.policy_results['y_proba']\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            y_pred = (y_proba >= thresh).astype(int)\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (cm[0,0], 0, 0, 0)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        # Precision-Recall curve\n",
    "        ax1.plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "        ax1.plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "        \n",
    "        # Mark policy scenarios\n",
    "        for scenario in scenarios:\n",
    "            thresh = results[scenario]['threshold']\n",
    "            ax1.axvline(thresh, color='gray', linestyle='--', alpha=0.5)\n",
    "            ax1.text(thresh, 0.95, scenario[0].upper(), ha='center', fontsize=9)\n",
    "        \n",
    "        ax1.set_xlabel('Decision Threshold')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.set_title('Precision-Recall vs Threshold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Precision-Recall tradeoff\n",
    "        ax2.plot(recalls, precisions, linewidth=2, color='purple')\n",
    "        ax2.set_xlabel('Recall')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.set_title('Precision-Recall Tradeoff Curve')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # Mark policy scenarios on PR curve\n",
    "        for scenario in scenarios:\n",
    "            p = results[scenario]['precision']\n",
    "            r = results[scenario]['recall']\n",
    "            ax2.scatter(r, p, s=100, zorder=5)\n",
    "            ax2.text(r, p+0.05, scenario[0].upper(), ha='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/threshold_sensitivity.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"  Saved: {output_dir}/threshold_sensitivity.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate_policy_report(self, output_path='reports/policy_comparison_report.txt'):\n",
    "        \"\"\"Generate text report summarizing policy comparison.\"\"\"\n",
    "        os.makedirs('reports', exist_ok=True)\n",
    "        \n",
    "        print(\"\\nGenerating policy comparison report...\")\n",
    "        \n",
    "        results = self.policy_results['scenarios']\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"POLICY COMPARISON REPORT\\n\")\n",
    "            f.write(\"Policy Risk Inference from Simulated Reports\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"This report compares three policy decision thresholds for risk\\n\")\n",
    "            f.write(\"classification in institutional reports.\\n\\n\")\n",
    "            \n",
    "            f.write(\"SCENARIOS EVALUATED\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            for name, scenario in self.policy_scenarios.items():\n",
    "                f.write(f\"\\n{name.upper()}\\n\")\n",
    "                f.write(f\"  Threshold: {scenario['threshold']}\\n\")\n",
    "                f.write(f\"  Description: {scenario['description']}\\n\")\n",
    "                f.write(f\"  Priority: {scenario['priority']}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\\nPERFORMANCE COMPARISON\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            for scenario_name, metrics in results.items():\n",
    "                f.write(f\"\\n{scenario_name.upper()} Policy\\n\")\n",
    "                f.write(f\"  Accuracy:  {metrics['accuracy']:.3f}\\n\")\n",
    "                f.write(f\"  Precision: {metrics['precision']:.3f}\\n\")\n",
    "                f.write(f\"  Recall:    {metrics['recall']:.3f}\\n\")\n",
    "                f.write(f\"  F1-Score:  {metrics['f1']:.3f}\\n\")\n",
    "                f.write(f\"  False Positives: {metrics['fp']}\\n\")\n",
    "                f.write(f\"  False Negatives: {metrics['fn']}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\\nKEY FINDINGS\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"1. High overall accuracy (~98%) is misleading due to class imbalance.\\n\")\n",
    "            f.write(\"2. Conservative policy catches more risks but generates more false alarms.\\n\")\n",
    "            f.write(\"3. Aggressive policy reduces false alarms but misses more true risks.\\n\")\n",
    "            f.write(\"4. Threshold selection should align with institutional risk tolerance.\\n\")\n",
    "            \n",
    "            f.write(\"\\n\\nRECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\"*70 + \"\\n\")\n",
    "            f.write(\"• High-stakes domains: Use conservative threshold\\n\")\n",
    "            f.write(\"• Resource constraints: Use aggressive threshold\\n\")\n",
    "            f.write(\"• General operations: Use balanced threshold\\n\")\n",
    "            f.write(\"• Consider cost-weighted evaluation metrics\\n\")\n",
    "            f.write(\"• Implement human review for borderline cases\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        \n",
    "        print(f\"  Saved: {output_path}\")\n",
    "        return self\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute full policy comparison pipeline.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"POLICY COMPARISON & INTERPRETATION PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # LOAD YOUR DATA HERE\n",
    "    data_path = 'data/processed/reports_with_features.csv'\n",
    "    print(f\"\\nLoading data from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    # LOAD YOUR MODELS HERE\n",
    "    model_dir = 'models'\n",
    "    models = {}\n",
    "    \n",
    "    print(f\"\\nLoading models from: {model_dir}\")\n",
    "    for model_file in os.listdir(model_dir):\n",
    "        if model_file.endswith('.pkl'):\n",
    "            model_name = model_file.replace('.pkl', '')\n",
    "            with open(f'{model_dir}/{model_file}', 'rb') as f:\n",
    "                models[model_name] = pickle.load(f)\n",
    "            print(f\"  Loaded: {model_name}\")\n",
    "    \n",
    "    # Initialize comparator\n",
    "    comparator = PolicyComparator(df, models)\n",
    "    \n",
    "    # Define and simulate policy scenarios\n",
    "    comparator.define_policy_scenarios()\n",
    "    comparator.simulate_policy_decisions('logistic_regression')\n",
    "    \n",
    "    # Generate comparative analysis\n",
    "    comparison_df = comparator.compare_scenarios()\n",
    "    \n",
    "    # Create visualizations\n",
    "    comparator.visualize_policy_comparison()\n",
    "    \n",
    "    # Generate report\n",
    "    comparator.generate_policy_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"POLICY ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nKey Outputs:\")\n",
    "    print(\"  - Comparison visualizations: figures/\")\n",
    "    print(\"  - Policy report: reports/policy_comparison_report.txt\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"  - Review threshold sensitivity analysis\")\n",
    "    print(\"  - Consider cost-sensitive learning approaches\")\n",
    "    print(\"  - Implement human-in-the-loop review for borderline cases\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc3d91-1271-42ac-bdb3-021c976d6722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
