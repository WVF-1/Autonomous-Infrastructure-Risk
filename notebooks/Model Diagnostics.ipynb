{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf3c346-a2b7-4d8c-b06e-c646a0c813b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL DIAGNOSTIC & FIX PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Loading data from: data/processed/reports_with_features_and_labels.csv\n",
      "Loaded 3000 rows with columns: ['id', 'timestamp', 'style', 'topic', 'sentiment', 'load_factor', 'agents', 'capacity', 'text', 'style_id', 'topic_id', 'sentiment_id', 'cleaned_text', 'risk_high_severity_count', 'risk_violation_count', 'risk_financial_count', 'risk_temporal_count', 'risk_density', 'text_length', 'composite_risk_score', 'risk_label', 'risk_label_from_sentiment', 'risk_label_from_load', 'risk_label_synthetic']\n",
      "Initialized diagnostic with 3000 reports\n",
      "\n",
      "======================================================================\n",
      "MODEL DIAGNOSTIC REPORT\n",
      "======================================================================\n",
      "\n",
      "Features being used: ['risk_high_severity_count', 'risk_violation_count', 'risk_financial_count', 'risk_temporal_count', 'risk_density', 'risk_label', 'risk_label_from_sentiment', 'risk_label_from_load', 'risk_label_synthetic']\n",
      "Number of features: 9\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "DIAGNOSTIC 1: CLASS IMBALANCE\n",
      "----------------------------------------------------------------------\n",
      "  Class 1:  3000 samples (100.00%)\n",
      "\n",
      "  Imbalance Ratio: 1.0:1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "DIAGNOSTIC 2: FEATURE DISCRIMINATIVE POWER\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  risk_high_severity_count:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0000 (std: 0.0000)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_violation_count:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0000 (std: 0.0000)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_financial_count:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0000 (std: 0.0000)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_temporal_count:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0000 (std: 0.0000)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_density:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0000 (std: 0.0000)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_label:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 1.0000 (std: 0.0000)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_label_from_sentiment:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0000 (std: 0.0000)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_label_from_load:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0013 (std: 0.0365)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "  risk_label_synthetic:\n",
      "    Low-risk mean:  nan (std: nan)\n",
      "    High-risk mean: 0.0200 (std: 0.1400)\n",
      "    Effect size (Cohen's d): nan\n",
      "    ✓✓ LARGE: Strong discriminative power\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "DIAGNOSTIC 3: FEATURE VARIANCE\n",
      "----------------------------------------------------------------------\n",
      "  risk_high_severity_count: variance = 0.000000\n",
      "    ⚠ Near-zero variance - feature may not be informative\n",
      "  risk_violation_count: variance = 0.000000\n",
      "    ⚠ Near-zero variance - feature may not be informative\n",
      "  risk_financial_count: variance = 0.000000\n",
      "    ⚠ Near-zero variance - feature may not be informative\n",
      "  risk_temporal_count: variance = 0.000000\n",
      "    ⚠ Near-zero variance - feature may not be informative\n",
      "  risk_density: variance = 0.000000\n",
      "    ⚠ Near-zero variance - feature may not be informative\n",
      "  risk_label: variance = 0.000000\n",
      "    ⚠ Near-zero variance - feature may not be informative\n",
      "  risk_label_from_sentiment: variance = 0.000000\n",
      "    ⚠ Near-zero variance - feature may not be informative\n",
      "  risk_label_from_load: variance = 0.001332\n",
      "  risk_label_synthetic: variance = 0.019607\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "DIAGNOSTIC 4: FEATURE-TARGET CORRELATION\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rfull\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\rfull\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  risk_high_severity_count: r = nan\n",
      "  risk_violation_count: r = nan\n",
      "  risk_financial_count: r = nan\n",
      "  risk_temporal_count: r = nan\n",
      "  risk_density: r = nan\n",
      "  risk_label: r = nan\n",
      "  risk_label_from_sentiment: r = nan\n",
      "  risk_label_from_load: r = nan\n",
      "  risk_label_synthetic: r = nan\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "DIAGNOSTIC 5: BASELINE MODEL PERFORMANCE\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 544\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 544\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[5], line 506\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# Run diagnostic\u001b[39;00m\n\u001b[0;32m    505\u001b[0m diagnostic \u001b[38;5;241m=\u001b[39m ModelDiagnostic(df)\n\u001b[1;32m--> 506\u001b[0m results \u001b[38;5;241m=\u001b[39m diagnostic\u001b[38;5;241m.\u001b[39mrun_full_diagnostic()\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Check if diagnostic succeeded\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[5], line 152\u001b[0m, in \u001b[0;36mModelDiagnostic.run_full_diagnostic\u001b[1;34m(self, target_col)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Simple logistic regression\u001b[39;00m\n\u001b[0;32m    151\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    154\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    155\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1224\u001b[0m     X,\n\u001b[0;32m   1225\u001b[0m     y,\n\u001b[0;32m   1226\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1227\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[0;32m   1228\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1229\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1230\u001b[0m )\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1304\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1305\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1306\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1307\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1308\u001b[0m     force_writeable\u001b[38;5;241m=\u001b[39mforce_writeable,\n\u001b[0;32m   1309\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1310\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1311\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1312\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1313\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1314\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    124\u001b[0m     X,\n\u001b[0;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Diagnostic and Fix Script\n",
    "================================\n",
    "Diagnoses why the model is not learning meaningful patterns and provides\n",
    "solutions to improve risk classification performance.\n",
    "\n",
    "Part of: Policy Risk Inference from Simulated Reports\n",
    "Author: William V. Fullerton\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "class ModelDiagnostic:\n",
    "    \"\"\"Diagnose and fix model performance issues.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize diagnostic with dataframe.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        print(f\"Initialized diagnostic with {len(self.df)} reports\")\n",
    "        \n",
    "    def run_full_diagnostic(self, target_col='risk_label'):\n",
    "        \"\"\"Run comprehensive diagnostic on data and features.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MODEL DIAGNOSTIC REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Find target column\n",
    "        if target_col not in self.df.columns:\n",
    "            label_cols = [col for col in self.df.columns if 'label' in col.lower() or 'target' in col.lower()]\n",
    "            if label_cols:\n",
    "                target_col = label_cols[0]\n",
    "                print(f\"Using target column: '{target_col}'\")\n",
    "            else:\n",
    "                print(\"ERROR: No target column found!\")\n",
    "                print(f\"Available columns: {list(self.df.columns)}\")\n",
    "                return None\n",
    "        \n",
    "        # Get features\n",
    "        risk_feature_cols = [col for col in self.df.columns if col.startswith('risk_')]\n",
    "        \n",
    "        if not risk_feature_cols:\n",
    "            print(\"ERROR: No risk features found!\")\n",
    "            print(f\"Available columns: {list(self.df.columns)}\")\n",
    "            print(\"\\nDid you run script 02 (language analysis) first?\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nFeatures being used: {risk_feature_cols}\")\n",
    "        print(f\"Number of features: {len(risk_feature_cols)}\")\n",
    "        \n",
    "        y = self.df[target_col].values\n",
    "        X = self.df[risk_feature_cols].values\n",
    "        \n",
    "        # DIAGNOSTIC 1: Class Distribution\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"DIAGNOSTIC 1: CLASS IMBALANCE\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            pct = 100 * count / len(y)\n",
    "            print(f\"  Class {label}: {count:5d} samples ({pct:.2f}%)\")\n",
    "        \n",
    "        imbalance_ratio = counts.max() / counts.min()\n",
    "        print(f\"\\n  Imbalance Ratio: {imbalance_ratio:.1f}:1\")\n",
    "        \n",
    "        if imbalance_ratio > 20:\n",
    "            print(\"  ⚠ SEVERE CLASS IMBALANCE DETECTED\")\n",
    "            print(\"  → This is likely causing the model to predict only the majority class\")\n",
    "        \n",
    "        # DIAGNOSTIC 2: Feature Statistics by Class\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"DIAGNOSTIC 2: FEATURE DISCRIMINATIVE POWER\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for feature in risk_feature_cols:\n",
    "            class_0_mean = self.df[self.df[target_col] == 0][feature].mean()\n",
    "            class_1_mean = self.df[self.df[target_col] == 1][feature].mean()\n",
    "            \n",
    "            class_0_std = self.df[self.df[target_col] == 0][feature].std()\n",
    "            class_1_std = self.df[self.df[target_col] == 1][feature].std()\n",
    "            \n",
    "            # Calculate separation (effect size)\n",
    "            pooled_std = np.sqrt((class_0_std**2 + class_1_std**2) / 2)\n",
    "            effect_size = abs(class_1_mean - class_0_mean) / (pooled_std + 1e-10)\n",
    "            \n",
    "            print(f\"\\n  {feature}:\")\n",
    "            print(f\"    Low-risk mean:  {class_0_mean:.4f} (std: {class_0_std:.4f})\")\n",
    "            print(f\"    High-risk mean: {class_1_mean:.4f} (std: {class_1_std:.4f})\")\n",
    "            print(f\"    Effect size (Cohen's d): {effect_size:.4f}\")\n",
    "            \n",
    "            if effect_size < 0.2:\n",
    "                print(f\"    ⚠ WEAK: Features barely differ between classes\")\n",
    "            elif effect_size < 0.5:\n",
    "                print(f\"    → SMALL: Some separation but limited\")\n",
    "            elif effect_size < 0.8:\n",
    "                print(f\"    ✓ MEDIUM: Reasonable discriminative power\")\n",
    "            else:\n",
    "                print(f\"    ✓✓ LARGE: Strong discriminative power\")\n",
    "        \n",
    "        # DIAGNOSTIC 3: Feature Variance\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"DIAGNOSTIC 3: FEATURE VARIANCE\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for feature in risk_feature_cols:\n",
    "            variance = self.df[feature].var()\n",
    "            print(f\"  {feature}: variance = {variance:.6f}\")\n",
    "            \n",
    "            if variance < 0.001:\n",
    "                print(f\"    ⚠ Near-zero variance - feature may not be informative\")\n",
    "        \n",
    "        # DIAGNOSTIC 4: Feature Correlation\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"DIAGNOSTIC 4: FEATURE-TARGET CORRELATION\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for feature in risk_feature_cols:\n",
    "            correlation = self.df[feature].corr(self.df[target_col])\n",
    "            print(f\"  {feature}: r = {correlation:.4f}\")\n",
    "            \n",
    "            if abs(correlation) < 0.05:\n",
    "                print(f\"    ⚠ Very weak correlation with target\")\n",
    "        \n",
    "        # DIAGNOSTIC 5: Baseline Model Check\n",
    "        print(\"\\n\" + \"-\"*70)\n",
    "        print(\"DIAGNOSTIC 5: BASELINE MODEL PERFORMANCE\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Simple logistic regression\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        print(\"\\n  Confusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        \n",
    "        # Check if model is predicting only one class\n",
    "        unique_preds = np.unique(y_pred)\n",
    "        if len(unique_preds) == 1:\n",
    "            print(f\"\\n  ⚠⚠ CRITICAL: Model only predicts class {unique_preds[0]}\")\n",
    "            print(\"  → Model has collapsed to majority class prediction\")\n",
    "        \n",
    "        # Check probability distribution\n",
    "        print(f\"\\n  Predicted probability statistics:\")\n",
    "        print(f\"    Min:  {y_proba.min():.6f}\")\n",
    "        print(f\"    Max:  {y_proba.max():.6f}\")\n",
    "        print(f\"    Mean: {y_proba.mean():.6f}\")\n",
    "        print(f\"    Std:  {y_proba.std():.6f}\")\n",
    "        \n",
    "        if y_proba.std() < 0.01:\n",
    "            print(f\"    ⚠ Very low variance in predictions\")\n",
    "            print(f\"    → Model is not confident in distinguishing classes\")\n",
    "        \n",
    "        # SUMMARY AND RECOMMENDATIONS\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"DIAGNOSTIC SUMMARY & RECOMMENDATIONS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nIdentified Issues:\")\n",
    "        \n",
    "        if imbalance_ratio > 20:\n",
    "            print(\"\\n  1. SEVERE CLASS IMBALANCE\")\n",
    "            print(\"     → Use SMOTE or class weighting\")\n",
    "            print(\"     → Consider cost-sensitive learning\")\n",
    "        \n",
    "        max_effect_size = max([\n",
    "            abs(self.df[self.df[target_col] == 1][f].mean() - \n",
    "                self.df[self.df[target_col] == 0][f].mean()) / \n",
    "            (self.df[f].std() + 1e-10)\n",
    "            for f in risk_feature_cols\n",
    "        ])\n",
    "        \n",
    "        if max_effect_size < 0.5:\n",
    "            print(\"\\n  2. WEAK FEATURE SEPARATION\")\n",
    "            print(\"     → Features don't strongly distinguish between classes\")\n",
    "            print(\"     → Need better feature engineering\")\n",
    "            print(\"     → Consider using TF-IDF directly or adding more features\")\n",
    "        \n",
    "        if len(unique_preds) == 1:\n",
    "            print(\"\\n  3. MODEL COLLAPSE\")\n",
    "            print(\"     → Model only predicting majority class\")\n",
    "            print(\"     → Apply solutions below immediately\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'imbalance_ratio': imbalance_ratio,\n",
    "            'max_effect_size': max_effect_size,\n",
    "            'model_collapsed': len(unique_preds) == 1,\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "\n",
    "\n",
    "class ModelFixer:\n",
    "    \"\"\"Apply fixes to improve model performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, diagnostic_results):\n",
    "        \"\"\"Initialize with diagnostic results.\"\"\"\n",
    "        self.results = diagnostic_results\n",
    "        self.X_train = diagnostic_results['X_train']\n",
    "        self.X_test = diagnostic_results['X_test']\n",
    "        self.y_train = diagnostic_results['y_train']\n",
    "        self.y_test = diagnostic_results['y_test']\n",
    "        self.models = {}\n",
    "        \n",
    "    def fix_with_smote(self):\n",
    "        \"\"\"Solution 1: Oversample minority class with SMOTE.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SOLUTION 1: SMOTE (Synthetic Minority Oversampling)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Apply SMOTE\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_smote, y_train_smote = smote.fit_resample(self.X_train, self.y_train)\n",
    "            \n",
    "            print(f\"\\nOriginal training set: {len(self.y_train)} samples\")\n",
    "            print(f\"After SMOTE: {len(y_train_smote)} samples\")\n",
    "            \n",
    "            unique, counts = np.unique(y_train_smote, return_counts=True)\n",
    "            for label, count in zip(unique, counts):\n",
    "                print(f\"  Class {label}: {count} samples\")\n",
    "            \n",
    "            # Train model\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            model.fit(X_train_smote, y_train_smote)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            y_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "            \n",
    "            print(\"\\nResults:\")\n",
    "            print(classification_report(self.y_test, y_pred, \n",
    "                                       target_names=['Low Risk', 'High Risk']))\n",
    "            \n",
    "            cm = confusion_matrix(self.y_test, y_pred)\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(cm)\n",
    "            \n",
    "            if len(np.unique(y_pred)) > 1:\n",
    "                auc = roc_auc_score(self.y_test, y_proba)\n",
    "                print(f\"\\nROC-AUC: {auc:.3f}\")\n",
    "                print(\"✓ Model is now predicting both classes!\")\n",
    "            \n",
    "            self.models['smote'] = {\n",
    "                'model': model,\n",
    "                'y_pred': y_pred,\n",
    "                'y_proba': y_proba,\n",
    "                'method': 'SMOTE Oversampling'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with SMOTE: {e}\")\n",
    "            print(\"Note: SMOTE requires at least 2 minority samples\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fix_with_class_weights(self):\n",
    "        \"\"\"Solution 2: Use class weights to penalize errors on minority class.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SOLUTION 2: CLASS WEIGHTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Calculate class weights\n",
    "        unique, counts = np.unique(self.y_train, return_counts=True)\n",
    "        total = len(self.y_train)\n",
    "        class_weights = {label: total / (len(unique) * count) \n",
    "                        for label, count in zip(unique, counts)}\n",
    "        \n",
    "        print(f\"\\nCalculated class weights:\")\n",
    "        for label, weight in class_weights.items():\n",
    "            print(f\"  Class {label}: {weight:.2f}\")\n",
    "        \n",
    "        # Train with class weights\n",
    "        model = LogisticRegression(\n",
    "            class_weight=class_weights,\n",
    "            random_state=42,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        y_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(classification_report(self.y_test, y_pred,\n",
    "                                   target_names=['Low Risk', 'High Risk']))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        if len(np.unique(y_pred)) > 1:\n",
    "            auc = roc_auc_score(self.y_test, y_proba)\n",
    "            print(f\"\\nROC-AUC: {auc:.3f}\")\n",
    "            print(\"✓ Model is now predicting both classes!\")\n",
    "        \n",
    "        self.models['class_weights'] = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred,\n",
    "            'y_proba': y_proba,\n",
    "            'method': 'Class Weights'\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fix_with_undersampling(self):\n",
    "        \"\"\"Solution 3: Undersample majority class.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SOLUTION 3: RANDOM UNDERSAMPLING\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Apply undersampling\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_train_under, y_train_under = rus.fit_resample(self.X_train, self.y_train)\n",
    "        \n",
    "        print(f\"\\nOriginal training set: {len(self.y_train)} samples\")\n",
    "        print(f\"After undersampling: {len(y_train_under)} samples\")\n",
    "        \n",
    "        unique, counts = np.unique(y_train_under, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"  Class {label}: {count} samples\")\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        model.fit(X_train_under, y_train_under)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        y_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(classification_report(self.y_test, y_pred,\n",
    "                                   target_names=['Low Risk', 'High Risk']))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        if len(np.unique(y_pred)) > 1:\n",
    "            auc = roc_auc_score(self.y_test, y_proba)\n",
    "            print(f\"\\nROC-AUC: {auc:.3f}\")\n",
    "            print(\"✓ Model is now predicting both classes!\")\n",
    "        \n",
    "        self.models['undersampling'] = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred,\n",
    "            'y_proba': y_proba,\n",
    "            'method': 'Random Undersampling'\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fix_with_gradient_boosting(self):\n",
    "        \"\"\"Solution 4: Use Gradient Boosting with scale_pos_weight.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SOLUTION 4: GRADIENT BOOSTING WITH WEIGHTED CLASSES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Calculate scale_pos_weight\n",
    "        unique, counts = np.unique(self.y_train, return_counts=True)\n",
    "        scale_pos_weight = counts[0] / counts[1] if len(counts) > 1 else 1.0\n",
    "        \n",
    "        print(f\"\\nScale pos weight: {scale_pos_weight:.2f}\")\n",
    "        \n",
    "        # Train model\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Manual sample weighting\n",
    "        sample_weights = np.ones(len(self.y_train))\n",
    "        sample_weights[self.y_train == 1] = scale_pos_weight\n",
    "        \n",
    "        model.fit(self.X_train, self.y_train, sample_weight=sample_weights)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        y_proba = model.predict_proba(self.X_test)[:, 1]\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(classification_report(self.y_test, y_pred,\n",
    "                                   target_names=['Low Risk', 'High Risk']))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        if len(np.unique(y_pred)) > 1:\n",
    "            auc = roc_auc_score(self.y_test, y_proba)\n",
    "            print(f\"\\nROC-AUC: {auc:.3f}\")\n",
    "            print(\"✓ Model is now predicting both classes!\")\n",
    "        \n",
    "        self.models['gradient_boosting'] = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred,\n",
    "            'y_proba': y_proba,\n",
    "            'method': 'Gradient Boosting (Weighted)'\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def compare_all_solutions(self):\n",
    "        \"\"\"Compare all solutions side-by-side.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SOLUTION COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        comparison_data = []\n",
    "        \n",
    "        for method_name, results in self.models.items():\n",
    "            y_pred = results['y_pred']\n",
    "            y_proba = results['y_proba']\n",
    "            \n",
    "            # Calculate metrics\n",
    "            cm = confusion_matrix(self.y_test, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (cm[0, 0], 0, 0, 0)\n",
    "            \n",
    "            accuracy = (tp + tn) / len(self.y_test)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            if len(np.unique(y_pred)) > 1:\n",
    "                auc = roc_auc_score(self.y_test, y_proba)\n",
    "            else:\n",
    "                auc = 0.5\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Method': results['method'],\n",
    "                'Accuracy': f'{accuracy:.3f}',\n",
    "                'Precision': f'{precision:.3f}',\n",
    "                'Recall': f'{recall:.3f}',\n",
    "                'F1-Score': f'{f1:.3f}',\n",
    "                'ROC-AUC': f'{auc:.3f}',\n",
    "                'FP': fp,\n",
    "                'FN': fn\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"RECOMMENDATION\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nBased on your use case, choose:\")\n",
    "        print(\"  • SMOTE: Best overall balance, creates synthetic examples\")\n",
    "        print(\"  • Class Weights: Fast, no data augmentation needed\")\n",
    "        print(\"  • Undersampling: When you have plenty of majority class data\")\n",
    "        print(\"  • Gradient Boosting: Often best performance for complex patterns\")\n",
    "        \n",
    "        return comparison_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run diagnostic and apply fixes.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL DIAGNOSTIC & FIX PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load data (output from script 02)\n",
    "    data_path = 'data/processed/reports_with_features_and_labels.csv'\n",
    "    print(f\"\\nLoading data from: {data_path}\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"ERROR: File not found: {data_path}\")\n",
    "        print(\"\\nPlease run script 02 (language analysis) first to generate this file.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} rows with columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Run diagnostic\n",
    "    diagnostic = ModelDiagnostic(df)\n",
    "    results = diagnostic.run_full_diagnostic()\n",
    "    \n",
    "    # Check if diagnostic succeeded\n",
    "    if results is None:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"DIAGNOSTIC FAILED\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nPlease check:\")\n",
    "        print(\"  1. Did you run script 02 first?\")\n",
    "        print(\"  2. Does your data have a target/label column?\")\n",
    "        print(\"  3. Does your data have risk features (columns starting with 'risk_')?\")\n",
    "        return\n",
    "    \n",
    "    # Apply fixes\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"APPLYING FIXES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fixer = ModelFixer(results)\n",
    "    fixer.fix_with_smote()\n",
    "    fixer.fix_with_class_weights()\n",
    "    fixer.fix_with_undersampling()\n",
    "    fixer.fix_with_gradient_boosting()\n",
    "    \n",
    "    # Compare solutions\n",
    "    comparison_df = fixer.compare_all_solutions()\n",
    "    \n",
    "    # Save comparison\n",
    "    os.makedirs('reports', exist_ok=True)\n",
    "    comparison_df.to_csv('reports/solution_comparison.csv', index=False)\n",
    "    print(\"\\nSaved comparison to: reports/solution_comparison.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DIAGNOSTIC COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8eb815-cc54-4b3b-9451-8c6fdd229e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
