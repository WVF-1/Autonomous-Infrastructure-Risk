{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968ed7db-a1b7-4a3b-871e-8bafa4807f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: C:\\Users\\rfull\\Building Data Together Weeklies\\Autonomous Infrastructure Risk\\data\\processed_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data/processed_dataset.csv\")\n",
    "\n",
    "print(f\"\\nLoading data from: {DATA_PATH.resolve()}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd8f50e2-22e5-451d-83f9-359419348d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REPORT LANGUAGE ANALYSIS PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Loading data from: data/processed_dataset.csv\n",
      "Loaded 3000 rows\n",
      "Initialized with 3000 reports\n",
      "Columns: ['id', 'timestamp', 'style', 'topic', 'sentiment', 'load_factor', 'agents', 'capacity', 'text', 'style_id', 'topic_id', 'sentiment_id']\n",
      "\n",
      "Preprocessing report texts...\n",
      "Warning: 'report_text' column not found.\n",
      "Available columns: ['id', 'timestamp', 'style', 'topic', 'sentiment', 'load_factor', 'agents', 'capacity', 'text', 'style_id', 'topic_id', 'sentiment_id']\n",
      "Using column: 'text' instead\n",
      "Extracting lexical risk features...\n",
      "Added 5 risk feature columns\n",
      "\n",
      "Extracting TF-IDF features (top 100)...\n",
      "TF-IDF matrix shape: (3000, 100)\n",
      "Sample features: ['09', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n",
      "\n",
      "Analyzing language patterns by risk_label...\n",
      "Warning: risk_label not found.\n",
      "Available columns: ['id', 'timestamp', 'style', 'topic', 'sentiment', 'load_factor', 'agents', 'capacity', 'text', 'style_id', 'topic_id', 'sentiment_id', 'cleaned_text', 'risk_high_severity_count', 'risk_violation_count', 'risk_financial_count', 'risk_temporal_count', 'risk_density']\n",
      "Using column: 'risk_high_severity_count' instead\n",
      "\n",
      "Risk label distribution:\n",
      "risk_high_severity_count\n",
      "0    3000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mean risk features by group:\n",
      "                          risk_high_severity_count  risk_violation_count  \\\n",
      "risk_high_severity_count                                                   \n",
      "0                                              0.0                   0.0   \n",
      "\n",
      "                          risk_financial_count  risk_temporal_count  \\\n",
      "risk_high_severity_count                                              \n",
      "0                                          0.0                  0.0   \n",
      "\n",
      "                          risk_density  \n",
      "risk_high_severity_count                \n",
      "0                                  0.0  \n",
      "\n",
      "Generating visualizations...\n",
      "Saved: figures/risk_features_by_class.png\n",
      "Saved: figures/text_length_distribution.png\n",
      "Saved: figures/top_tfidf_terms.png\n",
      "\n",
      "Saved processed data with features to: data/processed/reports_with_features.csv\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Key Outputs:\n",
      "  - Processed features: data/processed/reports_with_features.csv\n",
      "  - Visualizations: figures/\n",
      "\n",
      "Next Step: Run 03_risk_inference_model.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Handle text cleaning and normalization.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean and normalize report text.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep spaces and basic punctuation\n",
    "        text = re.sub(r'[^a-z0-9\\s\\.\\,\\!\\?]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_risk_lexicon_features(text):\n",
    "        \"\"\"Extract custom lexical risk markers from text.\"\"\"\n",
    "        risk_keywords = {\n",
    "            'high_severity': ['critical', 'severe', 'urgent', 'emergency', 'dangerous'],\n",
    "            'violation': ['violation', 'breach', 'non-compliant', 'unauthorized'],\n",
    "            'financial': ['loss', 'deficit', 'overrun', 'penalty', 'fine'],\n",
    "            'temporal': ['delayed', 'overdue', 'late', 'missed', 'behind']\n",
    "        }\n",
    "        \n",
    "        features = {}\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for category, keywords in risk_keywords.items():\n",
    "            count = sum(text_lower.count(keyword) for keyword in keywords)\n",
    "            features[f'risk_{category}_count'] = count\n",
    "        \n",
    "        # Total risk keyword density\n",
    "        total_words = len(text.split())\n",
    "        total_risk_words = sum(features.values())\n",
    "        features['risk_density'] = total_risk_words / max(total_words, 1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "class LanguageAnalyzer:\n",
    "    \"\"\"Perform comprehensive language analysis on report corpus.\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"Initialize analyzer with dataframe.\"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        print(f\"Initialized with {len(self.df)} reports\")\n",
    "        print(f\"Columns: {list(self.df.columns)}\")\n",
    "        \n",
    "    def preprocess_reports(self, text_column='report_text'):\n",
    "        \"\"\"Clean and normalize all report texts.\"\"\"\n",
    "        print(\"\\nPreprocessing report texts...\")\n",
    "        preprocessor = TextPreprocessor()\n",
    "        \n",
    "        # Check if text column exists\n",
    "        if text_column not in self.df.columns:\n",
    "            print(f\"Warning: '{text_column}' column not found.\")\n",
    "            print(f\"Available columns: {list(self.df.columns)}\")\n",
    "            # Try to find a text-like column\n",
    "            text_cols = [col for col in self.df.columns if 'text' in col.lower() or 'report' in col.lower()]\n",
    "            if text_cols:\n",
    "                text_column = text_cols[0]\n",
    "                print(f\"Using column: '{text_column}' instead\")\n",
    "            else:\n",
    "                print(\"Error: Cannot find text column\")\n",
    "                return self\n",
    "        \n",
    "        # Clean text\n",
    "        self.df['cleaned_text'] = self.df[text_column].apply(\n",
    "            preprocessor.clean_text\n",
    "        )\n",
    "        \n",
    "        # Extract lexical risk features\n",
    "        print(\"Extracting lexical risk features...\")\n",
    "        risk_features = self.df['cleaned_text'].apply(\n",
    "            preprocessor.extract_risk_lexicon_features\n",
    "        )\n",
    "        risk_df = pd.DataFrame(risk_features.tolist())\n",
    "        \n",
    "        # Merge with main dataframe\n",
    "        self.df = pd.concat([self.df, risk_df], axis=1)\n",
    "        \n",
    "        print(f\"Added {len(risk_df.columns)} risk feature columns\")\n",
    "        return self\n",
    "    \n",
    "    def extract_tfidf_features(self, max_features=100):\n",
    "        \"\"\"Extract TF-IDF features from cleaned text.\"\"\"\n",
    "        print(f\"\\nExtracting TF-IDF features (top {max_features})...\")\n",
    "        \n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(\n",
    "            self.df['cleaned_text']\n",
    "        )\n",
    "        \n",
    "        print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        print(f\"Sample features: {list(feature_names[:10])}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def analyze_language_by_risk(self, risk_column='risk_label'):\n",
    "        \"\"\"Analyze language patterns grouped by risk level.\"\"\"\n",
    "        print(f\"\\nAnalyzing language patterns by {risk_column}...\")\n",
    "        \n",
    "        if risk_column not in self.df.columns:\n",
    "            print(f\"Warning: {risk_column} not found.\")\n",
    "            print(f\"Available columns: {list(self.df.columns)}\")\n",
    "            # Try to find a risk/label column\n",
    "            label_cols = [col for col in self.df.columns if 'risk' in col.lower() or 'label' in col.lower()]\n",
    "            if label_cols:\n",
    "                risk_column = label_cols[0]\n",
    "                print(f\"Using column: '{risk_column}' instead\")\n",
    "            else:\n",
    "                print(\"Creating dummy labels for demonstration...\")\n",
    "                self.df[risk_column] = np.random.choice([0, 1], size=len(self.df), p=[0.98, 0.02])\n",
    "        \n",
    "        # Risk distribution\n",
    "        risk_dist = self.df[risk_column].value_counts()\n",
    "        print(f\"\\nRisk label distribution:\")\n",
    "        print(risk_dist)\n",
    "        \n",
    "        # Compare lexical features by risk group\n",
    "        risk_features = [col for col in self.df.columns if col.startswith('risk_')]\n",
    "        \n",
    "        if risk_features:\n",
    "            print(f\"\\nMean risk features by group:\")\n",
    "            feature_comparison = self.df.groupby(risk_column)[risk_features].mean()\n",
    "            print(feature_comparison)\n",
    "        else:\n",
    "            print(\"No risk features found to compare\")\n",
    "            feature_comparison = None\n",
    "        \n",
    "        return feature_comparison\n",
    "    \n",
    "    def visualize_language_patterns(self, risk_column='risk_label', output_dir='figures'):\n",
    "        \"\"\"Create visualizations of language patterns.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nGenerating visualizations...\")\n",
    "        \n",
    "        # Make sure risk column exists\n",
    "        if risk_column not in self.df.columns:\n",
    "            label_cols = [col for col in self.df.columns if 'risk' in col.lower() or 'label' in col.lower()]\n",
    "            if label_cols:\n",
    "                risk_column = label_cols[0]\n",
    "            else:\n",
    "                self.df[risk_column] = np.random.choice([0, 1], size=len(self.df), p=[0.98, 0.02])\n",
    "        \n",
    "        # 1. Risk feature distributions by class\n",
    "        risk_features = [col for col in self.df.columns if col.startswith('risk_')]\n",
    "        \n",
    "        if risk_features:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for idx, feature in enumerate(risk_features[:4]):\n",
    "                if idx < len(axes):\n",
    "                    self.df.boxplot(column=feature, by=risk_column, ax=axes[idx])\n",
    "                    axes[idx].set_title(f'{feature} by Risk Level')\n",
    "                    axes[idx].set_xlabel('Risk Label')\n",
    "                    axes[idx].set_ylabel(feature)\n",
    "            \n",
    "            plt.suptitle('Lexical Risk Features by Risk Class', y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/risk_features_by_class.png', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved: {output_dir}/risk_features_by_class.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        # 2. Text length distribution\n",
    "        self.df['text_length'] = self.df['cleaned_text'].str.split().str.len()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for label in self.df[risk_column].unique():\n",
    "            subset = self.df[self.df[risk_column] == label]['text_length']\n",
    "            plt.hist(subset, alpha=0.6, bins=30, label=f'Risk={label}')\n",
    "        \n",
    "        plt.xlabel('Text Length (words)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Report Length Distribution by Risk Class')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{output_dir}/text_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {output_dir}/text_length_distribution.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Top TF-IDF terms\n",
    "        if self.tfidf_matrix is not None:\n",
    "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "            mean_tfidf = np.asarray(self.tfidf_matrix.mean(axis=0)).flatten()\n",
    "            top_indices = mean_tfidf.argsort()[-20:][::-1]\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(range(20), mean_tfidf[top_indices])\n",
    "            plt.yticks(range(20), [feature_names[i] for i in top_indices])\n",
    "            plt.xlabel('Mean TF-IDF Score')\n",
    "            plt.title('Top 20 TF-IDF Terms Across Corpus')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/top_tfidf_terms.png', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved: {output_dir}/top_tfidf_terms.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_processed_dataframe(self):\n",
    "        \"\"\"Return the processed dataframe with all features.\"\"\"\n",
    "        return self.df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute full language analysis pipeline.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"REPORT LANGUAGE ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # LOAD YOUR DATA HERE\n",
    "    # Replace this path with your actual file path\n",
    "    data_path = 'data/processed_dataset.csv'\n",
    "    \n",
    "    print(f\"\\nLoading data from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    # Initialize analyzer with your dataframe\n",
    "    analyzer = LanguageAnalyzer(df)\n",
    "    \n",
    "    # Run analysis pipeline\n",
    "    analyzer.preprocess_reports()\n",
    "    analyzer.extract_tfidf_features(max_features=100)\n",
    "    \n",
    "    # Analyze by risk group\n",
    "    feature_comparison = analyzer.analyze_language_by_risk()\n",
    "    \n",
    "    # Generate visualizations\n",
    "    analyzer.visualize_language_patterns()\n",
    "    \n",
    "    # Get processed dataframe\n",
    "    processed_df = analyzer.get_processed_dataframe()\n",
    "    \n",
    "    # Save results\n",
    "    output_path = 'data/processed/reports_with_features.csv'\n",
    "    os.makedirs('data/processed', exist_ok=True)\n",
    "    processed_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved processed data with features to: {output_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nKey Outputs:\")\n",
    "    print(\"  - Processed features: data/processed/reports_with_features.csv\")\n",
    "    print(\"  - Visualizations: figures/\")\n",
    "    print(\"\\nNext Step: Run 03_risk_inference_model.py\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59181aa4-8baa-4b0a-995d-9dea7b396edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
